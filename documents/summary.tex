\documentclass[12pt]{article}
\title{\vspace{-2.0cm}Summary}
\begin{document}
    \date{\vspace{-5ex}}
    \maketitle
    The importance and popularity of neural networks and deeplearning have been rapidly increasing in the past decades,
    and they are becoming the default tool to tackle challenging problems in more and more fields of machine
    learning, reaching state-of-art results.\\
    \indent One of most researched topics of machine learning is time series analysis and prediction where
    classical machine learning methods and deeplearning techniques have been trading blows to offer the most reliable
    and accurate solutions.
    While time series can be sorted into plenty of categories, each having their own intricacies,
    financial market predictions has proved to be a notoriously hard problem due to several reasons.
    Regarding the data,its very noisy (especially for short periods) and there are countless (external) factors,
    some of which are random, such as external political or economic events that usually cannot be foreseen (e.g.\
    Tesla announces to sell most of their bitcoin holdings).
    It also typically has changing mean and variance, additionally the patterns that were observed one time,
    may not apply for the future - if there is a pattern, people will start abusing it, causing the pattern itself to change.
    These above make the preprocessing stage quite difficult to perform properly.\\
    \indent A recent innovations in deeplearning architectures often implement an attention mechanism which makes
    it possible for the neural network to focus on different parts of the input when creating different parts of the output.
    This can be very useful when labeling objects on a picture (concentrating only around the context of the object to decide
    whether it is a dog or a cat) or for example when translating a sentence and the next word is heavily dependent only
    on a subset of words in the sentence to be translated.
    The backbone of most state-of-art solutions nowadays to image- or translation-related problems is
    the Transformer model which uses the attention mechanism really effectively.\\
    \indent In my work, I investigate the accuracy of the latter architecture when it comes to time series classification
    ,namely trying to predict the near future trend of bitcoin prices using data from the bitcoin-domain as well as
    from social media activity since the cryptomarket is very sensitive to hype and general talk about it.
    I also implement some popular model types from classical machine learning and deeplearning to help me better evaluate
    the performance of the Transformer-inspired model.
\end{document}
